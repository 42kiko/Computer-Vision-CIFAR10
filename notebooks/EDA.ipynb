{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîß Imports & setup\n",
    "\n",
    "This section prepares all required libraries, plotting configuration and helper utilities\n",
    "used throughout the CIFAR-10 evaluation notebook.# Imports & Setup"
   ],
   "id": "2bbf0bc81379d926"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "from src.utils import (\n",
    "    load_cifar10,\n",
    "    CLASS_NAMES,\n",
    "    CLASS_NAMES_EMOJI,\n",
    "    predict_classes,\n",
    "    evaluate_model,\n",
    "    confusion_matrix_array,\n",
    "    load_model,\n",
    "    load_history,\n",
    "    classification_report_str,\n",
    "    save_fig,\n",
    "    upscale_and_super_sharpen\n",
    ")\n",
    "\n",
    "# Use a dark theme for all Plotly figures\n",
    "pio.templates.default = \"plotly_dark\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üì¶ Load model, history & CIFAR-10 test data\n",
    "\n",
    "Here we load the trained CNN model, its saved training history and the CIFAR-10\n",
    "test split that we will analyse in detail.# Model, History & Test Data"
   ],
   "id": "b247d15814c7ee24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load trained model\n",
    "model = load_model(\"cifar10_main\")\n",
    "\n",
    "history = load_history(\"cifar10_main\")\n",
    "\n",
    "data = load_cifar10(normalize=False)\n",
    "\n",
    "print(\"Test images:\", data.x_test.shape, data.x_test.dtype)\n",
    "print(\"Test labels:\", data.y_test.shape, data.y_test.dtype)"
   ],
   "id": "39d9703f743d8297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìâ Visualise training curves\n",
    "\n",
    "The following plots show training and validation accuracy and loss over the epochs.\n",
    "They help to understand convergence speed and to detect potential overfitting or underfitting."
   ],
   "id": "806035c536ca09c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_metrics = evaluate_model(model, data.x_test, data.y_test)\n",
    "print(f\"Test loss:     {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "epochs_range = list(range(1, len(history_dict[\"loss\"]) + 1))\n",
    "\n",
    "fig_acc = go.Figure()\n",
    "fig_acc.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_range,\n",
    "        y=history_dict[\"accuracy\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Train accuracy\",\n",
    "    )\n",
    ")\n",
    "fig_acc.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_range,\n",
    "        y=history_dict[\"val_accuracy\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Val accuracy\",\n",
    "    )\n",
    ")\n",
    "fig_acc.update_layout(\n",
    "    title=\"Training and validation accuracy\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    ")\n",
    "fig_acc.show()\n",
    "\n",
    "fig_loss = go.Figure()\n",
    "fig_loss.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_range,\n",
    "        y=history_dict[\"loss\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Train loss\",\n",
    "    )\n",
    ")\n",
    "fig_loss.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_range,\n",
    "        y=history_dict[\"val_loss\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Val loss\",\n",
    "    )\n",
    ")\n",
    "fig_loss.update_layout(\n",
    "    title=\"Training and validation loss\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Loss\",\n",
    ")\n",
    "fig_loss.show()\n",
    "\n",
    "save_fig(fig_acc, \"cifar10_acc\")\n",
    "save_fig(fig_loss, \"cifar10_loss\")"
   ],
   "id": "e22fa05a24a796e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ Evaluate on the test set\n",
    "\n",
    "We evaluate the trained model on the held-out CIFAR-10 test set and inspect the\n",
    "overall loss and accuracy to see how well the model generalises."
   ],
   "id": "7fd53408b2bfc40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate on test data\n",
    "metrics = evaluate_model(model, data.x_test, data.y_test)\n",
    "print(f\"Test loss:     {metrics['loss']:.4f}\")\n",
    "print(f\"Test accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Predict test labels\n",
    "y_test_pred = predict_classes(model, data.x_test)\n",
    "\n",
    "# Classification report (precision / recall / F1 per class)\n",
    "report_str = classification_report_str(data.y_test, y_test_pred, target_names=CLASS_NAMES_EMOJI)\n",
    "print(report_str)"
   ],
   "id": "77cc2e59491ac49a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üî¢ Confusion matrix\n",
    "\n",
    "The normalised confusion matrix reveals which classes the model confuses with each other.\n",
    "Bright values on the diagonal are correct predictions, off-diagonal values indicate systematic errors.# Confusion Matrix"
   ],
   "id": "51747fb70c13353d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_test_pred = predict_classes(model, data.x_test)\n",
    "\n",
    "cm_norm = confusion_matrix_array(data.y_test, y_test_pred, normalize=True)\n",
    "\n",
    "fig_cm = px.imshow(\n",
    "    cm_norm,\n",
    "    x=CLASS_NAMES_EMOJI,\n",
    "    y=CLASS_NAMES_EMOJI,\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    labels={\"x\": \"Predicted class\", \"y\": \"True class\", \"color\": \"Proportion\"},\n",
    "    title=\"Normalized confusion matrix (CIFAR-10)\",\n",
    ")\n",
    "fig_cm.update_xaxes(tickfont=dict(size=18))\n",
    "fig_cm.update_yaxes(tickfont=dict(size=18))\n",
    "fig_cm.update_xaxes(side=\"top\")\n",
    "fig_cm.show()\n",
    "\n",
    "save_fig(fig_cm,\"cifar10_confusion_matrix\")"
   ],
   "id": "f97134d00357be21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß† Build prediction analysis DataFrame\n",
    "\n",
    "In this step we collect model outputs (probabilities, predicted labels and correctness flags)\n",
    "into a single `analysis_df` DataFrame that powers all subsequent analyses and plots.## Analyse"
   ],
   "id": "3d0a224eb566c438"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "probs = model.predict(data.x_test, batch_size=128, verbose=1)\n",
    "\n",
    "y_true = data.y_test\n",
    "y_pred = probs.argmax(axis=1)\n",
    "prob_pred = probs.max(axis=1)\n",
    "prob_true = probs[np.arange(len(y_true)), y_true]\n",
    "\n",
    "analysis_df = pd.DataFrame(\n",
    "    {\n",
    "        \"idx\": np.arange(len(y_true)),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"prob_pred\": prob_pred,\n",
    "        \"prob_true\": prob_true,\n",
    "    }\n",
    ")\n",
    "analysis_df[\"correct\"] = analysis_df[\"y_true\"] == analysis_df[\"y_pred\"]\n",
    "analysis_df[\"true_name\"] = analysis_df[\"y_true\"].map(lambda i: CLASS_NAMES_EMOJI[int(i)])\n",
    "analysis_df[\"pred_name\"] = analysis_df[\"y_pred\"].map(lambda i: CLASS_NAMES_EMOJI[int(i)])\n",
    "\n",
    "analysis_df.head()"
   ],
   "id": "801762afb683c22b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìä Per-class accuracy & confidence\n",
    "\n",
    "Here we measure how well the model performs for each class individually and how confident\n",
    "it is about the true class on average. This highlights particularly strong and weak classes.## Per-Class Accuracy & Confidence"
   ],
   "id": "ab378bf50f7958a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Per-class Accuracy (%)\n",
    "per_class_acc = (\n",
    "    analysis_df\n",
    "    .groupby(\"true_name\")[\"correct\"]\n",
    "    .mean()\n",
    "    .mul(100.0)\n",
    "    .reset_index(name=\"accuracy_percent\")\n",
    "    .sort_values(\"accuracy_percent\", ascending=False)\n",
    ")\n",
    "per_class_acc"
   ],
   "id": "a78654cd83daaf49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig_acc_per_class = px.bar(\n",
    "    per_class_acc,\n",
    "    x=\"true_name\",\n",
    "    y=\"accuracy_percent\",\n",
    "    title=\"Per-class accuracy on CIFAR-10\",\n",
    "    labels={\"true_name\": \"True class\", \"accuracy_percent\": \"Accuracy (%)\"},\n",
    ")\n",
    "fig_acc_per_class.update_xaxes(tickfont=dict(size=28))\n",
    "fig_acc_per_class.update_layout()\n",
    "fig_acc_per_class.show()\n",
    "\n",
    "save_fig(fig_acc_per_class,\"cifar10_per_class_accuracy\")"
   ],
   "id": "f90816eea82ef485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "true_conf_stats = (\n",
    "    analysis_df\n",
    "    .groupby(\"true_name\")[\"prob_true\"]\n",
    "    .agg([\"mean\", \"median\"])\n",
    "    .reset_index()\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "true_conf_stats"
   ],
   "id": "eedd9621edae765e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig_true_conf = px.bar(\n",
    "    true_conf_stats,\n",
    "    x=\"true_name\",\n",
    "    y=\"mean\",\n",
    "    title=\"Average confidence for the true class (all predictions)\",\n",
    "    labels={\"true_name\": \"True class\", \"mean\": \"Avg probability for true class\"},\n",
    ")\n",
    "fig_true_conf.update_xaxes(tickfont=dict(size=28))\n",
    "fig_true_conf.update_layout()\n",
    "fig_true_conf.show()\n",
    "\n",
    "save_fig(fig_true_conf,\"cifar10_avg_confidence_per_true_class\")"
   ],
   "id": "8402168c6f4c6626",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üèÜ Top 1% most confident correct predictions\n",
    "\n",
    "We visualise the most confidently correct predictions to see which images the model\n",
    "is absolutely sure about and how these \"easy\" examples look.# Top 1 % right"
   ],
   "id": "1d22abf81942c0fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TOP_FRACTION = 0.01\n",
    "\n",
    "correct_df = analysis_df[analysis_df[\"correct\"]].copy()\n",
    "n_top_correct = max(1, int(len(correct_df) * TOP_FRACTION))\n",
    "\n",
    "top_confident_correct = (\n",
    "    correct_df\n",
    "    .sort_values(\"prob_pred\", ascending=False)\n",
    "    .head(n_top_correct)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Top {TOP_FRACTION*100:.1f}% most confident CORRECT predictions: \"\n",
    "    f\"{len(top_confident_correct)} samples\"\n",
    ")\n",
    "top_confident_correct[[\"idx\", \"true_name\", \"pred_name\", \"prob_pred\", \"prob_true\"]].head()"
   ],
   "id": "6b4cd90f90f6db76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "N_SHOW = min(25, len(top_confident_correct))\n",
    "rows = math.ceil(N_SHOW / 5)\n",
    "cols = 5\n",
    "\n",
    "fig_top = make_subplots(\n",
    "    rows=rows,\n",
    "    cols=cols,\n",
    "    subplot_titles=[\n",
    "        f\"true={row.true_name}<br> pred={row.pred_name} {row.prob_pred*100:.1f}%\"\n",
    "        for _, row in top_confident_correct.iloc[:N_SHOW].iterrows()\n",
    "    ],\n",
    ")\n",
    "\n",
    "for i, (_, row) in enumerate(top_confident_correct.iloc[:N_SHOW].iterrows()):\n",
    "    r = i // cols + 1\n",
    "    c = i % cols + 1\n",
    "    img = data.x_test[row.idx]\n",
    "\n",
    "    img_up = upscale_and_super_sharpen(img, scale=8)\n",
    "\n",
    "    fig_top.add_trace(\n",
    "        go.Image(z=img_up),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "for ann in fig_top.layout.annotations:\n",
    "    ann.font.size = 18\n",
    "\n",
    "fig_top.update_layout(\n",
    "    height=300 * rows,\n",
    "    width=300 * cols,\n",
    "    title=dict(\n",
    "        text=\"Top 1% most confident CORRECT predictions\",\n",
    "        font=dict(size=18),\n",
    "        y=0.98,\n",
    "        yanchor=\"top\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig_top.for_each_xaxis(\n",
    "    lambda ax: ax.update(\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "    )\n",
    ")\n",
    "fig_top.for_each_yaxis(\n",
    "    lambda ax: ax.update(\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_top.show()\n",
    "\n",
    "save_fig(fig_top,\"cifar10_top_1_percent_correct_predictions\")"
   ],
   "id": "7746c59a92041536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚ö†Ô∏è Top 1% most confident wrong predictions\n",
    "\n",
    "Next we inspect the most confidently wrong predictions. These are especially interesting,\n",
    "because the model is very sure but still incorrect ‚Äì useful for understanding failure modes."
   ],
   "id": "5d28d5e91cd882e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wrong_df = analysis_df[~analysis_df[\"correct\"]].copy()\n",
    "n_top_wrong = max(1, int(len(wrong_df) * TOP_FRACTION))\n",
    "\n",
    "top_confident_wrong = (\n",
    "    wrong_df\n",
    "    .sort_values(\"prob_pred\", ascending=False)\n",
    "    .head(n_top_wrong)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Top {TOP_FRACTION*100:.1f}% most confident WRONG predictions: \"\n",
    "    f\"{len(top_confident_wrong)} samples\"\n",
    ")\n",
    "top_confident_wrong[[\"idx\", \"true_name\", \"pred_name\", \"prob_pred\", \"prob_true\"]].head()"
   ],
   "id": "eb695728bfd8735",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N_SHOW_WRONG = min(25, len(top_confident_wrong))\n",
    "rows = math.ceil(N_SHOW_WRONG / 5)\n",
    "cols = 5\n",
    "\n",
    "fig_wrong = make_subplots(\n",
    "    rows=rows,\n",
    "    cols=cols,\n",
    "    subplot_titles=[\n",
    "        f\"true={row.true_name}<br>pred={row.pred_name} {row.prob_pred*100:.1f}%\"\n",
    "        for _, row in top_confident_wrong.iloc[:N_SHOW_WRONG].iterrows()\n",
    "    ],\n",
    ")\n",
    "\n",
    "for i, (_, row) in enumerate(top_confident_wrong.iloc[:N_SHOW_WRONG].iterrows()):\n",
    "    r = i // cols + 1\n",
    "    c = i % cols + 1\n",
    "    img = data.x_test[row.idx]\n",
    "\n",
    "    img_up = upscale_and_super_sharpen(img, scale=8)\n",
    "\n",
    "    fig_wrong.add_trace(\n",
    "        go.Image(z=img_up),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "for ann in fig_wrong.layout.annotations:\n",
    "    ann.font.size = 18\n",
    "\n",
    "fig_wrong.update_layout(\n",
    "    height=300 * rows,\n",
    "    width=300 * cols,\n",
    "    title=dict(\n",
    "        text=\"Top 1% most confident WRONG predictions\",\n",
    "        font=dict(size=18),\n",
    "        y=0.98,\n",
    "        yanchor=\"top\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig_wrong.for_each_xaxis(\n",
    "    lambda ax: ax.update(\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "    )\n",
    ")\n",
    "fig_wrong.for_each_yaxis(\n",
    "    lambda ax: ax.update(\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_wrong.show()\n",
    "\n",
    "save_fig(fig_wrong,\"cifar10_top_1_percent_wrong_predictions\")"
   ],
   "id": "6a747ab79a023c7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üéØ Confidence distribution: correct vs wrong\n",
    "\n",
    "This plot compares the model‚Äôs confidence for the predicted class between **correct**\n",
    "and **wrong** predictions. Ideally, correct predictions should concentrate at high\n",
    "probabilities, while wrong predictions should appear mostly at lower confidence levels.\n",
    "Large overlap indicates overconfident errors."
   ],
   "id": "a9bfc6359a8b259a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig_conf_hist = go.Figure()\n",
    "\n",
    "fig_conf_hist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=analysis_df.loc[analysis_df[\"correct\"], \"prob_pred\"],\n",
    "        nbinsx=20,\n",
    "        name=\"Correct\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_conf_hist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=analysis_df.loc[~analysis_df[\"correct\"], \"prob_pred\"],\n",
    "        nbinsx=20,\n",
    "        name=\"Wrong\",\n",
    "        opacity=0.6,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_conf_hist.update_layout(\n",
    "    barmode=\"overlay\",\n",
    "    title=\"Confidence distribution for predicted class (correct vs wrong)\",\n",
    "    xaxis_title=\"Probability of predicted class\",\n",
    "    yaxis_title=\"Count\",\n",
    ")\n",
    "fig_conf_hist.show()\n",
    "\n",
    "save_fig(fig_conf_hist,\"cifar10_confidence_hist\")"
   ],
   "id": "6aa278f0933024ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "af6e870e17f96d98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "THRESHOLD = 0.3\n",
    "\n",
    "hard_but_correct = (\n",
    "    analysis_df\n",
    "    .loc[analysis_df[\"correct\"] & (analysis_df[\"prob_true\"] < THRESHOLD)]\n",
    "    .sort_values(\"prob_true\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Found {len(hard_but_correct)} 'hard but correct' samples with prob_true < {THRESHOLD}\")\n",
    "\n",
    "hard_but_correct[[\"idx\", \"true_name\", \"pred_name\", \"prob_pred\", \"prob_true\"]].head()"
   ],
   "id": "ad3fcfa0484a2b3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "44a8332ec7733b13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "N_SHOW_HARD = min(25, len(hard_but_correct))\n",
    "if N_SHOW_HARD == 0:\n",
    "    print(\"No hard-but-correct samples found for this threshold.\")\n",
    "else:\n",
    "    rows = math.ceil(N_SHOW_HARD / 5)\n",
    "    cols = 5\n",
    "\n",
    "    fig_hard = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=[\n",
    "            f\"{row.true_name}<br>p_true={row.prob_true*100:.1f}%\"\n",
    "            for _, row in hard_but_correct.iloc[:N_SHOW_HARD].iterrows()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, (_, row) in enumerate(hard_but_correct.iloc[:N_SHOW_HARD].iterrows()):\n",
    "        r = i // cols + 1\n",
    "        c = i % cols + 1\n",
    "        img = data.x_test[row.idx]\n",
    "\n",
    "        img_up = upscale_and_super_sharpen(img, scale=8)\n",
    "\n",
    "        fig_hard.add_trace(\n",
    "            go.Image(z=img_up),\n",
    "            row=r,\n",
    "            col=c,\n",
    "        )\n",
    "\n",
    "\n",
    "    for ann in fig_hard.layout.annotations:\n",
    "        ann.font.size = 20\n",
    "\n",
    "    fig_hard.update_layout(\n",
    "        height=300 * rows,\n",
    "        width=300 * cols,\n",
    "        title=dict(\n",
    "            text=f\"Hard but correct (prob_true < {THRESHOLD})\",\n",
    "            font=dict(size=18),\n",
    "            y=0.98,\n",
    "            yanchor=\"top\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig_hard.for_each_xaxis(\n",
    "        lambda ax: ax.update(\n",
    "            showticklabels=False,\n",
    "            showgrid=False,\n",
    "            zeroline=False,\n",
    "        )\n",
    "    )\n",
    "    fig_hard.for_each_yaxis(\n",
    "        lambda ax: ax.update(\n",
    "            showticklabels=False,\n",
    "            showgrid=False,\n",
    "            zeroline=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig_hard.show()\n",
    "\n",
    "    save_fig(fig_hard,\"cifar10_hard_prediction_grid\")"
   ],
   "id": "c6bc1816f9a6e4fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß™ Additional error analysis\n",
    "\n",
    "In the final analysis we look at confidence distributions, \"hard but correct\" examples\n",
    "and per-class misclassification patterns to better understand where the model struggles."
   ],
   "id": "131aaf6b3d739d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wrong_df = analysis_df[~analysis_df[\"correct\"]].copy()\n",
    "\n",
    "# Counts pro (true_name, pred_name)\n",
    "confusions_all = (\n",
    "    wrong_df\n",
    "    .groupby([\"true_name\", \"pred_name\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# pro true_name top_k h√§ufigste pred_name behalten\n",
    "top_k = 5\n",
    "confusions_all[\"rank\"] = confusions_all.groupby(\"true_name\")[\"count\"].rank(\n",
    "    method=\"first\", ascending=False\n",
    ")\n",
    "top_confusions_all = confusions_all[confusions_all[\"rank\"] <= top_k].copy()\n",
    "\n",
    "top_confusions_all.head()"
   ],
   "id": "d4f5e65c7dd40467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig_all_conf = px.bar(\n",
    "    top_confusions_all,\n",
    "    x=\"pred_name\",\n",
    "    y=\"count\",\n",
    "    color=\"pred_name\",\n",
    "    facet_col=\"true_name\",\n",
    "    facet_col_wrap=5,\n",
    "    facet_row_spacing=0.15,\n",
    "    facet_col_spacing=0.06,\n",
    "    title=f\"Top {top_k} misclassifications per true class\",\n",
    "    labels={\n",
    "        \"pred_name\": \"Predicted class\",\n",
    "        \"count\": \"Number of errors\",\n",
    "        \"true_name\": \"True class\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Facet-Titel aufr√§umen und gr√∂√üer machen\n",
    "for ann in fig_all_conf.layout.annotations:\n",
    "    if isinstance(ann.text, str) and \"=\" in ann.text:\n",
    "        ann.text = ann.text.split(\"=\", 1)[1].strip()\n",
    "    ann.font.size = 28\n",
    "\n",
    "fig_all_conf.update_layout(\n",
    "    showlegend=False,\n",
    "    height=650,\n",
    "    title=dict(\n",
    "        font=dict(size=18),\n",
    "        y=0.98,\n",
    "        yanchor=\"top\",\n",
    "    ),\n",
    "    margin=dict(t=80, b=80, l=60, r=20),\n",
    ")\n",
    "\n",
    "fig_all_conf.for_each_xaxis(\n",
    "    lambda axis: axis.update(\n",
    "        tickfont=dict(size=14),\n",
    "        title_font=dict(size=14),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_all_conf.for_each_yaxis(\n",
    "    lambda axis: axis.update(\n",
    "        tickfont=dict(size=12),\n",
    "        title_font=dict(size=14),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_all_conf.show()\n",
    "\n",
    "save_fig(fig_all_conf,\"cifar10_misclassification_grid\")"
   ],
   "id": "3c9990727f1271e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe3fb84ffc7bf54d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
